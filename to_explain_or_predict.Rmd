---
title: "To Explain or Predict: how different modelling objectives change how you use the same tools"
author: "Chris Mainey"
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: "%current%"
      ratio: 16:9
    seal: false
---

```{r setup, include=FALSE}
library(ragg)

ragg_png = function(..., res = 192) {
  ragg::agg_png(..., res = res, units = "in")
}

#library(Cairo)

library(citr)
library(ggplot2)
library(ggforce)

options(htmltools.dir.version = FALSE)


knitr::opts_chunk$set(fig.width=5.8, fig.height=3, fig.align = "center", dev = "ragg_png", fig.ext = "png", dpi=300
                      #dev.args = list(png = list(type = "cairo"))
                      , fig.retina=3) 


## Set ggplot defaults
theme_set(
  theme_bw() +
    theme(legend.position = "right",
          plot.subtitle = element_text(face = "italic"
                                       , family = "sans"
                                       , size=10))
)


alpha <- 5.4776
X<-c(1,2,2)
Y<-alpha + (X * 1.2507)
Y[2] <- Y[1]
dt2  <- data.frame(X,Y)

triangle<- data.frame(X=c(1.5,2.13,0),
Y = c(6.3, 7.3, 6.2), 
#value=c("1", "1.5", "2"),
label= c("1", "\u03B2", "\u03B1" ))

library(reticulate)

use_virtualenv("exp_or_pred")

```



.pull-left[

<br><br>

# To explain or predict:
### How different modelling objectives change how you use the same tools

<br><br>


`r icons::icon_style(icons::fontawesome("envelope"))` [c.mainey1@nhs.net](mailto:c.mainey1@nhs.net)
`r icons::icon_style(icons::fontawesome("github"))` [chrismainey](https://github.com/chrismainey)
`r icons::icon_style(icons::fontawesome("linkedin"), fill = "#005EB8")`  [chrismainey](https://www.linkedin.com/in/chrismainey/)
`r icons::icon_style(icons::fontawesome("orcid"), fill = "#A6CE39")` [0000-0002-3018-6171](https://orcid.org/0000-0002-3018-6171)
]

.pull-right[

<p style="text-align:center;font-weight:bold;"><img src="man/figures/DALLE2024-11-08.webp" style="height:550px;" alt="A cartoon image generated by Chat-GPT and Dall-E 3 about the differences between explanatory and predictive modelling. Many of the words are nonsensical and illustrates the error inherent in the modelling">
<br>
Generated by Chat-GPT and Dall-E 3.  <br>
Any thoughts on the prediction error here?`  </p>
]

---

# Overview



This talk draws heavily on Professor Galit Shmueli's 2010 paper of the same name, which can be found:

>Shmueli, G. (2010), To Explain or To Predict?, Statistical Science, vol 25 no 3, pp. 289-310.


--
<br><br><br>
<center><h2>
I'm going to repeated ask you: what is your question?
</h2></center>

---

# The two broad classes of DS/modelling question:


1. Explain

  * How does an explanatory variable(s) relate to an outcome?
  * We might consider two main groups:
    * Description
    * Causal Inference / Counter factual analysis



2. Prediction

  * How can predictor variable(s) predict an outcome?
  * At future time points or in different context?



__You can use many of the same models to fit in either context, but how you do it is different!__

???

This applies whether you are bayesian / frequentist / 

---

# Example

We will use an example I sourced from Kaggle, related to the paper: 

Chicco, D., Jurman, G. Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone. _BMC Med Inform Decis Mak_ __20__, 16 (2020). https://doi.org/10.1186/s12911-020-1023-5

https://www.kaggle.com/datasets/andrewmvd/heart-failure-clinical-data/data

--

## Data related to heart failure

Paper tests various prediction methods to see if albumin and serum creatinine alone can predict death.

__I will be using logistic regression in both context__

---

# Regression in 2 minutes...

```{r lmsetup, include=FALSE}
set.seed(222)
x <- rnorm(50, 10, 4)
y <- runif(50, min = 0.5, 10) + (1.25*x)
z <- predict(lm(y~x))
set.seed(222)
library(ggplot2)
```

```{r lm1, echo=FALSE, fig.align="center"}
x <- rnorm(50, 10, 4)
y <- runif(50, min = 0.5, 10) + (1.25*x)

a<-ggplot(data.frame(x,y,z), aes(x=x,y=y))+
  geom_point(col="dodgerblue2", alpha=0.65)
a
```

---
### Regression models (1)

Regression gives us more options than correlation:

```{r lm3, echo=FALSE, fig.align="center", fig.retina=2, message=FALSE, warning=FALSE}
z <- lm(y~x)
print(a<- a + geom_smooth(method="lm", col="red", linetype=2,fullrange=TRUE, se=FALSE))
```


$$y= \alpha + \beta x + \epsilon$$

---

### Regression models (2)
Zooming in...
 
```{r lm35, echo=FALSE, fig.align="center", fig.retina=2, message=FALSE, warning=FALSE}
z <- lm(y~x)
print(a + geom_mark_circle(aes(y=5.4776 , x=0), col = "darkgoldenrod2", linetype="dashed",
             size=1.2)+
        geom_polygon(aes(x=X,y=Y), fill=NA, col = "darkgoldenrod2", linetype="dashed",
             size=1.2 ,data=dt2) +
        geom_label(aes(x=X, y = Y, label=label), data=triangle)+
        coord_cartesian(xlim = c(0,3),ylim= c(5,10))
        )
```

---

## Regression equation

$$\large{y= \alpha + \beta_{i} x_{i} + \epsilon}$$
<br>
.pull-left[
+ $y$ - is our 'outcome', or 'dependent' variable
+ $\alpha$ - is the 'intercept', the point where our line crosses y-axis
+ $\beta$ - is a coefficient (weight) applied to $x$ 
]
.pull-right[
+ $x$ - is our 'predictor', or 'independent' variable
+ $i$ - is our index, we can have $i$ predictor variables, each with a coefficient
+ $\epsilon$ - is the remaining ('residual') error
]

## Generalized linear model:

$$\large{g(\mu)= \alpha + \beta_{i} x_{i}}$$
Where $\mu$ is the _expectation_ of $Y$, and $g$ is the link function

So, for logistic regression, the link function is 'logit' or the log odds of the event.

---

---

# Explanatory model - R

```{r exp1_r_setup, include=FALSE}
library(readr)
heart_failure_dt <- read_csv("data/heart_failure_clinical_records_dataset.csv")

```

```{r mod1_exp}
model1 <- glm(DEATH_EVENT ~ serum_creatinine + ejection_fraction
              , data=heart_failure_dt
              , family = "binomial")

summary(model1)

ModelMetrics::auc(model1)
```

---

# Explanaory model - Python

```{python}
import pandas as pd

heart_failure_pd = pd.read_csv('./data/heart_failure_clinical_records_dataset.csv')

```
---

# Predictive Model - R

```{r }
model1_exp <- glm(DEATH_EVENT ~ serum_creatinine + ejection_fraction
              ,  data=heart_failure_dt
              , family = "binomial")

summary(model1_exp)

ModelMetrics::auc(model1_exp)
```


```{r mod1_exp}
trainIndex <- caret::createDataPartition(heart_failure_dt$DEATH_EVENT
                                         , p = .8
                                         , list = FALSE
                                         , times = 1)

Train <- heart_failure_dt[ trainIndex,]
Test  <- heart_failure_dt[-trainIndex,]

model1_pred <- glm(DEATH_EVENT ~ serum_creatinine + ejection_fraction
              ,  data=Train
              , family = "binomial")

predictions <- predict(model1_pred, newdata=Test, type="response")
# Model performance metrics

ModelMetrics::auc(Test$DEATH_EVENT, predictions)
```

---

## Predictive Model - R bonus

```{r ridge}
heart_failure_dt$sc_serum_creatinin <- scale(heart_failure_dt$serum_creatinine)
heart_failure_dt$sc_ejection_fraction <- scale(heart_failure_dt$ejection_fraction)

Train <- heart_failure_dt[ trainIndex,]
Test  <- heart_failure_dt[-trainIndex,]

x <- model.matrix(DEATH_EVENT~sc_serum_creatinin+sc_ejection_fraction, Train)[,-1]
y <- Train$DEATH_EVENT

library(glmnet)
# Cross validate to get best lambda (shrinkage)
cv <- cv.glmnet(x, y, alpha = 0, family="binomial")

ridge1<-glmnet(x,y, alpha=0, lamda=cv$lambda.min, family="binomial")

# Make predictions on the test data
x.test <- model.matrix(DEATH_EVENT~sc_serum_creatinin+sc_ejection_fraction, Test)[,-1]

predictions <- predict(ridge1, newx=x.test, type="response") |> as.vector()

ModelMetrics::auc(Test$DEATH_EVENT, predictions)
```


---

# What is important in assessing the fit:

.pull-left[

### Explanation
* Plausible relationship - DAG
* Understanding measurement error / bias
* Performance on whole dataset
* Feature engineering should be logically consistent with relationship
* Scale/centre predictors for interpretation reasons

]

pull-left[

### Prediction
* Prediction error on new data (new sample, hold-out/test set, cross-validation)
* Less concerned with interpreting coefficients / predictors
* Feature engineering can be extensive and esoteric
* Scale/centre predictors as good practise for computation reasons

]


So you might leave multiple 'non-significant' predictors in an explanatory model, as they rational and all effects conditional on each other

---


# Explain or predict Bingo:

1. Forecasting attendances at an Emergency Department

--

__Prediction!__

--

1. What drives people to attend and Emergency Department?

--

__Explanation!__


--

1. What is a person's risk of attending and Emergency Department?

--

__It depends...is it about the person's individual risk, or is it a risk-adjustment around a particular factor?__

--

1. Building a Large Language Model to answer questions

__Prediction__

--

1. Simulating long-term population health state changes

-- 

__It depends...:  are you testing what causes it, or predicting future states of the population?__

---

# References

Hernán, M. A., Hsu, J. and Healy, B. (2019) ‘A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks’, CHANCE, 32(1), pp. 42–49. doi: 10.1080/09332480.2019.1579578.
